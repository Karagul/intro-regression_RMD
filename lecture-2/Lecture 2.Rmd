---
title: "Lecture 2"
author: "Jeremy Albright - Methods"
date: "Monday, February 01, 2016"
output: 
  ioslides_presentation:
    css: ../css/style.css
    keep_md: yes
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE, fig.align='center')

library(dplyr)
library(ggplot2)
library(tibble)



```

## Null Hypothesis Significance Testing

20th century statistical analysis focused on inferring population characteristics on the basis of a sample.

Data were expensive to collect, while computers were limited by memory and processing speeds.

So we take a sample and analyze that, then generalize back to the population.

```{r samppop, out.width = "600px"}
knitr::include_graphics("Lecture_2_files/figure-html/SampPop.png")
```


## Null Hypothesis Significance Testing

The problem with sampling: Our sample estimate will almost always be wrong.

There is the true population _parameter_, and then there is our sample _statistic_.  

The distance between our estimated statistic and the population parameter is known as _sampling error_.

```{r error, out.width = "450"}
norm_data <- data_frame(x = seq(-4,4,by=.001), y = dnorm(seq(-4,4,by=.001)))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() + theme(axis.ticks = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank()) + 
  labs(x = "Sample Distribution", y = "") + geom_hline(aes(yintercept=0), linetype = "dashed") +
  geom_segment(aes(x = 0, y = -.01, xend = 0, yend = .02), size = 1.25, color = "firebrick") + 
  geom_text(x = 0, y = .03, label = paste("Sample Mean"), color = "firebrick") +
  geom_segment(aes(x = -1.5, y = -.01, xend = -1.5, yend = .02), size = 1.25, color = "firebrick") + 
  geom_text(x = -1.5, y = .03, label = paste("Population Mean"), color = "firebrick")


```


## Null Hypothesis Significance Testing

A sample mean is almost surely not the population value we really want to know.

The interesting thing is that, while we know we are wrong, we can actually say something about how wrong we are likely to be!

This is, for example, the reason why we see survey results published with a _margin of error_.

Our ability to quantify our level of wrongness comes from in large part from a result known as the _Central Limit Theorem_.  

This result works if we think of our sample as one of all possible samples of size $n$ that can be taken from a population.


## The Central Limit Theorem

Take a population distribution:

```{r slide1, out.width = "800px"}
knitr::include_graphics("Lecture_2_files/figure-html/Slide1.PNG")
```

## The Central Limit Theorem

Draw a sample, calculate the mean.

```{r slide2, out.width = "800px"}
knitr::include_graphics("Lecture_2_files/figure-html/Slide2.PNG")
```

## The Central Limit Theorem

Then draw another sample, calculate the mean.

```{r slide3, out.width = "800px"}
knitr::include_graphics("Lecture_2_files/figure-html/Slide3.PNG")
```


## The Central Limit Theorem

Do it again and again and again.

```{r slide4, out.width = "800px"}
knitr::include_graphics("Lecture_2_files/figure-html/Slide4.PNG")
```

## The Central Limit Theorem

Do it again and again and again.

```{r slide5, out.width = "800px"}
knitr::include_graphics("Lecture_2_files/figure-html/Slide5.PNG")
```

## The Central Limit Theorem

Do it again and again and again.

```{r slide6, out.width = "800px"}
knitr::include_graphics("Lecture_2_files/figure-html/Slide6.PNG")
```

## The Central Limit Theorem

Now think of these individual means as data points.  They can form a distribution called _the distribution of means_.

More generally, we call this the _sampling distribution_.  

```{r dist_means, out.width = "600px"}
norm_data <- data_frame(x = seq(-4,4,by=.001), y = dnorm(seq(-4,4,by=.001)))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() + theme(axis.ticks = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank()) + 
  labs(x = "Sample Means", y = "Frequency of Occurrence") + geom_hline(aes(yintercept=0), linetype = "dashed") 

```


## The Central Limit Theorem

Not the difference:

- Sample distribution: The distribution of individual data points from one sample.
- Sampling distribution: The distribution of a statistic calculated on repeated samples.

The CLT corresponds to the latter and provides essential information.  The first is fundamental.

> As the sample size $n$ gets larger, the sampling distribution of the mean (and many other statistics) converges to a normal distribution.


## The Central Limit Theorem

This result does not depend on the distribution of scores in the population.  Take, for example, the distribution of students across grades in a high school.

```{r students, out.width = "650px"}
students <- data_frame(x = c(rep(9,200),rep(10,200),rep(11,200),rep(12,200)))

ggplot(data = students, aes(x = x)) + geom_bar(fill = "firebrick", color = "black") +
  labs(x = "Grade", y = "Number of Students")

st_mean <- round(mean(students$x), 3)
st_std  <- round(sd(students$x), 3)
```

## The Central Limit Theorem

The mean of this population is $\mu = `r st_mean`$, and the standard deviation is $\sigma = `r st_std`$.

Now let's draw 1000 different random samples of size 50 from this population of students and calculate the mean grade for each sample.

Then we'll create a histogram to view the distribution of means from the repeated samples.

## The Central Limit Theorem

```{r sample}
set.seed(12345)
means <- sapply(1:1000, function(x) mean(sample_n(students, 50)$x))

```

| Sample | Mean  | 
|:------:|:------:|
| 1       | `r means[1]` |
| 2       | `r means[2]` |
| 3       | `r means[3]` |
| $\vdots$ | $\vdots$    |
| 998    | `r means[998]` |
| 999    | `r means[999]` |
| 1000    | `r means[1000]` |

## The Central Limit Theorem

```{r dist_samp}
means_df <- data_frame(x = means)

ggplot(data = means_df, aes(x = x)) + geom_bar(fill = "firebrick", color = "black") +
  labs(x = "Sample Mean", y = "Number of Samples")

```


## The Central Limit Theorem

There is more.  While the CLT tells us the shape of the distribution, it is also relatively easy to show that

1. The mean of the sampling distribution is the same as the population mean.  That is, the average sample mean is the true population mean.
2. The standard deviation of the sampling distribution, which we call the _standard error_, is equal to $\sigma/\sqrt(n)$.

In other words, the amount of sample-to-sample variability depends on the standard deviation of population scores and the size of the sample we draw.


## The Central Limit Theorem

For our student example, the population mean was `r st_mean` and the standard deviation was `r st_std`.

We therefore expect the sampling distribution of means to be centered on `r st_mean` with a standard error of 
$$
\frac{`r st_std`}{\sqrt{50}} = `r round(st_std/sqrt(50),3)`
$$
In fact, we get

* The mean of the sample means is `r round(mean(means),3)`.
* The standard error is `r round(sd(means),3)`




## Hypothesis Testing

If I know the population distribution, I will know the shape of the sampling distribution (provided $n$ is sufficiently large).

With this information, I can say how likely it is that a single sample was generated from a given population

Example: I know the population distribution of IQs has a mean of 100 and a standard deviation of 15.  

A "study group" of 100 undergraduates snorts some Adderall before taking the IQ test.

What is the probability that Adderall sniffing has no effect on IQ scores?

## Hypothesis Testing

If Adderall has no effect, then the mean IQ among Adderall sniffers should be the same as non-sniffers.  

If Adderall has an effect, then Adderall sniffers come from a different population with its own unique mean.  What should we infer?

We know that the distribution of means for IQ given $n = 100$ will:

1. Be normal
2. Be centered on 100 (the population mean)
3. Have a standard error equal to $\frac{15}{\sqrt{100}} = 1.5$.


## Hypothesis Testing

Our sample has a mean IQ of 103.  How likely is it that we would have observed this value if our sample were no different from the population?

```{r iq_plot}
xbar <- 103
xmin <- 93
xmax <- 107
mu   <- 100
se   <- 15/sqrt(100)

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)
poly_data <- data_frame(x_poly = c(xbar, subset_x, xmax), y_poly = c(0, dnorm(subset_x, mu, se), 0))


ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density") + geom_hline(aes(yintercept=0), linetype = "dashed") +
  geom_segment(aes(x = xbar+.5, y = .01, xend = xbar+.5, yend = .04), size = 1.25) + 
  geom_text(x = xbar+.6, y = .05, label = paste("p =", 1 - round(pnorm(xbar, mu, se),3)))

```



## Hypothesis Testing

We now have the basis for hypothesis testing.

We specify a(n assumed) known state of the world. We then want to assess whether our sample belongs to this state.

1. The assumed state is known as the _null hypothesis_.
2. Given a null hypothesis, we can - by the CLT - generate the _sampling distribution_.
3. We compare our sample statistic to the sampling distribution.
4. If our statistic is sufficiently unlikely given the null hypothesis, we reject the null.

This begs the question, what is _sufficiently unlikely_?


## Hypothesis Testing

We specify a certain level of acceptable error, which we refer to as $\alpha$ (alpha).

By convention, a typical level for $\alpha$ is .05.  What this means is that we are willing to accept the possibility that 5% of the time we will incorrectly reject the null hyopthesis.

For our IQ example, we know what the sampling distribution will look like.  If $\alpha$ = .05:


## Hypothesis Testing
```{r alpha05}
xbar <- qnorm(.95, mu, se)
subset_x <- x[x > xbar]

poly_data <- data_frame(x_poly = c(xbar, subset_x, xmax), y_poly = c(0, dnorm(subset_x, mu, se), 0))


ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density") +
  geom_segment(aes(x = xbar+.5, y = .02, xend = xbar+.5, yend = .07), size = 1.25) + 
  geom_text(x = xbar+.6, y = .08, label = paste("p =", 1 - round(pnorm(xbar, mu, se),3)))

```


## Hypothesis Testing

But what if we thought our sample was from a population the performed _worse_ on the IQ test?
```{r alpha05lower}
xbar <- qnorm(.05, mu, se)
subset_x <- x[x < xbar]

poly_data <- data_frame(x_poly = c(xmin, subset_x, xbar), y_poly = c(0, dnorm(subset_x, mu, se), 0))


ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density") +
  geom_segment(aes(x = xbar-.5, y = .02, xend = xbar-.5, yend = .07), size = 1.25) + 
  geom_text(x = xbar-.6, y = .08, label = paste("p =", round(pnorm(xbar, mu, se),3)))

```

## Hypothesis Testing

What if we're ambivalent?

```{r alpha05both}
xbar1 <- qnorm(.025, mu, se)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- qnorm(.975, mu, se)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density") +
  geom_segment(aes(x = xbar1-.5, y = .01, xend = xbar1-.5, yend = .05), size = 1.25) + 
  geom_text(x = xbar1-.5, y = .06, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = xbar2+.5, y = .01, xend = xbar2+.5, yend = .05), size = 1.25) + 
  geom_text(x = xbar2+.5, y = .06, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3)))


```

## Hypothesis Testing

We can make a couple of choices about what evidence is _good enough_ to reject the null hypothesis.

1. Set $\alpha$: Smaller values mean less of a chance of mistakenly rejecting the null hypothesis.
2. Choose a one-sided or two-sided test.

By convention, set $\alpha = .05$ and use a two-tailed test.

$\alpha$ is also called the _Type-1 Error Rate_, where a Type-1 error occurs when you mistakenly reject the null hypothesis.

Note that making $\alpha$ smaller reduces Type-1 errors, but it also makes it harder to _correctly_ reject a null hypothsis.

Failure to reject a null when you should is called a _Type-2 Error_.


## Hypothesis Testing Example

Let's say I think IQ test performance is better after exercise.  I take a sample of 25 individuals and have them exercise for an hour before taking the test.

The mean IQ test following exercise is 104.

I set $\alpha = .05$ and specify that I will perform a two-tailed test.  Can I reject the null?

Note that the sample size changed, so the standard error is now $\frac{15}{\sqrt{25}} = 3$.  This means the sampling distribution is a little wider than before.

## Hypothesis Testing Example
```{r exercise}
xmin <- 90
xmax <- 110
mu   <- 100
se   <- 15/sqrt(25)

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)

xbar1 <- qnorm(.025, mu, se)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- qnorm(.975, mu, se)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density") +
  geom_segment(aes(x = xbar1-.5, y = .01, xend = xbar1-.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar1-.5, y = .04, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = xbar2+.5, y = .01, xend = xbar2+.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar2+.5, y = .04, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3)))


```

## Hypothesis Testing Example
```{r exercise2}
xbar <- 104

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density") +
  geom_segment(aes(x = xbar1-.5, y = .01, xend = xbar1-.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar1-.5, y = .04, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = xbar2+.5, y = .01, xend = xbar2+.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar2+.5, y = .04, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3))) +
  geom_segment(aes(x = xbar+.6, y = 0, xend = xbar+.6, yend = .06), size = 1.25) + 
  geom_text(x = xbar+.5, y = .065, label = "Sample Mean")

```

## Hypothesis Testing Example

Conclusion: Do not reject the null hypothesis.

But compare what happens if we had a sample size of 100.  In that case, the standard error would be $\frac{15}{\sqrt{100}} = 1.5$

Because $SE = \frac{\sigma}{\sqrt{n}}$, increasing $n$ -- the sample size -- will reduce sampling variability.

In other words, the more data we have, the greater certainty we can have in an estimate. 

## Hypothesis Testing Example
```{r exercise3}
xbar <- 104

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density", title = "N = 25") +
  geom_segment(aes(x = xbar1-.5, y = .01, xend = xbar1-.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar1-.5, y = .04, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = xbar2+.5, y = .01, xend = xbar2+.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar2+.5, y = .04, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3))) +
  geom_segment(aes(x = xbar+.6, y = 0, xend = xbar+.6, yend = .06), size = 1.25) + 
  geom_text(x = xbar+.5, y = .065, label = "Sample Mean")

```


## Hypothesis Testing Example
```{r exercise4}
xbar <- 104

xmin <- 90
xmax <- 110
mu   <- 100
se   <- 15/sqrt(100)

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)

xbar1 <- qnorm(.025, mu, se)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- qnorm(.975, mu, se)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density", title = "N = 100") +
  geom_segment(aes(x = xbar1-.5, y = .01, xend = xbar1-.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar1-.5, y = .04, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = xbar2+.5, y = .01, xend = xbar2+.5, yend = .03), size = 1.25) + 
  geom_text(x = xbar2+.5, y = .04, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3))) +
  geom_segment(aes(x = xbar+.6, y = 0, xend = xbar+.6, yend = .06), size = 1.25) + 
  geom_text(x = xbar+.5, y = .065, label = "Sample Mean")

```


## Hypothesis Testing Example

Notice that in each of these graphs we denote the area under the curve in the tails with $p$, for probability of observing a statistic at least this far from the null hypothesis population mean.

This is the _p-value_.  We want $p$ to be less than $\alpha$.

Note that, since this is a two-tailed test, we add up the area in the tails on both sides of the distribution.  You will see

1. When $n = 25$, $p = .182 > .05$.
2. When $n = 100$, $p = .008 < .05$.

## Hypothesis Testing Example

```{r exercise5}
xbar <- 104

xmin <- 90
xmax <- 110
mu   <- 100
se   <- 15/sqrt(25)

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)

xbar1 <- 96
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- 104
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density", title = "N = 25") +
  geom_segment(aes(x = 93, y = .005, xend = 93, yend = .03), size = 1.25) + 
  geom_text(x = 93, y = .04, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = 107, y = .005, xend = 107, yend = .03), size = 1.25) + 
  geom_text(x = 107, y = .04, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3))) 

```

## Hypothesis Testing Example
```{r exercise6}
xbar <- 104

xmin <- 90
xmax <- 110
mu   <- 100
se   <- 15/sqrt(100)

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)

xbar1 <- 96
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- 104
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density", title = "N = 100") +
  geom_segment(aes(x = 95.5, y = 0, xend = 95.5, yend = .03), size = 1.25) + 
  geom_text(x = 95.5, y = .04, label = paste("p =", round(pnorm(xbar1, mu, se),3))) +
  geom_segment(aes(x = 104.5, y = 0, xend = 104.5, yend = .03), size = 1.25) + 
  geom_text(x = 104.5, y = .04, label = paste("p =", 1 - round(pnorm(xbar2, mu, se),3))) 

```


## Means as Z-Statistics

Recall from the last lecture that we can convert scores on any scale (with any mean and standard deviation) to $z$ scores, which have a zero mean and standard deviation of 1.  We used the formula

$$
z = \frac{x - \mu}{\sigma}
$$

We can do the same with means.  Let $M$ be a sample mean, $\mu$ the population means, and SE be the standard error, $\sigma/\sqrt{n}$.  Then

$$
z = \frac{M - \mu}{\text{SE}}
$$

## Means as Z-Statistics

For the IQ example with $M = 104$ and $n = 25$, SE = $15/\sqrt{25} = 3$.  Converted to a $z$ statistic,

$$
z = \frac{104 - 100}{3} = 1.333.
$$

Had $n$ been 100, then SE = $15/\sqrt{100} = 1.5$, meaning 

$$
z = \frac{104 - 100}{1.5} = 2.667.
$$



## Means as Z-Statistics

```{r z1}
xbar <- 1.33

xmin <- -4
xmax <- 4
mu   <- 0
se   <- 1

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)

xbar1 <- -1.96
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- 1.96
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density", title = "N = 25") +
  geom_segment(aes(x = xbar, y = 0, xend = xbar, yend = .04), size = 1.25) + 
  geom_text(x = xbar, y = .05, label = "z  = 1.333")

```

## Means as Z-Statistics
```{r z2}
xbar <- 2.67

xmin <- -4
xmax <- 4
mu   <- 0
se   <- 1

x = seq(xmin,xmax, by = .01)
y = dnorm(x, mu, sd = se)
subset_x <- x[x > xbar]

norm_data <- data_frame(x = x, y = y)

xbar1 <- -1.96
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dnorm(subset_x1, mu, se), 0))

xbar2 <- 1.96
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dnorm(subset_x2, mu, se), 0))

ggplot(norm_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "Sample Means Under Null Hypothesis", y = "Density", title = "N = 100") +
  geom_segment(aes(x = xbar, y = 0, xend = xbar, yend = .04), size = 1.25) + 
  geom_text(x = xbar, y = .05, label = "z  = 2.667")

```


## Means as Z-Statistics

Why bother with this transformation?

- Historically, needed to rely on tables to find the area under the sampling distribution curve. 
- But there is an infinite number of normal distributions.
- By making this conversion, the sampling distribution will always be the _unit_ or _standard normal_ distribution.
- Then you just need one table.  
    - Calculate the statistic.
    - Divide by standard error to get $z$.
    - Find associated area in the tail(s) of the standard normal distribution.

This is also how statistical software outputs results.


## Review

Before covering one last topic, make sure you understand the following:

1. If we know the mean $\mu$ and standard deviation $\sigma$ of a population, we know exactly what the sampling distribution of the mean will look like across repeated samples of size $n$.  
    + It will be normal.
    + It will be centered on zero.
    + It will have a standard deviation (more specifically, standard error) equal to $\frac{\sigma}{\sqrt{n}}$.
2. We can therefore assert a null hypothesis and know what the sampling distribution looks like given the null.


## Review

Furthermore, 

1. If our sample mean $M$ is sufficiently unlikely under the null hypothesis, we reject the null.
2. _Sufficiently unlikey_ is below a probability threshold, $\alpha$, usually set at .05 and distributed between the upper and lower tails of the distribution.
3. We can convert our mean to a $z$ statistic if that makes it easier to find the associated $p$-value.

This rests on knowing $\mu$ and $\sigma$.  What if we don't have enough information about the null population to know $\sigma$?

## The t distribution

We rarely know $\sigma$.  This means we can't know the sampling distribution without some further assumptions.

We _assume_ that our sample can provide an unbiased estimate of the population variance/standard deviation.  

Equivalently, we are saying that, even if our sample comes from a different population with a different mean, the variances in both populations are the same.

Recall our unbiased estimate of $\sigma$ is

$$
s = \frac{\sum_{i}^{N} \left(x_i - M\right)^2}{n - 1}
$$

where we call $n-1$ the _degrees of freedom_.

## The t distribution

So now we have an estimate of $\sigma$ that is unbiased.  We can now determine the standard error of the sampling distribution by substituting our estimate in for $\sigma$,

$$
\text{SE} = \frac{s}{\sqrt{n}}
$$

However, the fact that $s$ is just an _estimate_ means we have some added uncertainty in our hypothesis testing.  The normal distribution won't do.

Instead, we turn to the $t$ distribution.


## The t distribution

More specifically, there is no single $t$ distribution.  Instead, there is a different $t$ distribution depending on what our degrees of freedom are.

Since $df = n - 1$, this is equivalent to saying that the $t$ distribution we consider changes depending on what $n$ is.

When $n$ is small, the $t$ distribution we use will have fatter tails than the normal.  This captures the added uncertainty we have in our hypothesis testing -- it makes it a little harder to declare something significant -- when we have to make a guess as to what $\sigma$ is.

To illustrate:

## The t distribution

```{r t1}
x <- rep(seq(-5,5, by = .01),6)
y <- c(dt(seq(-5,5, by = .01), df = 2),
       dt(seq(-5,5, by = .01), df = 5),
       dt(seq(-5,5, by = .01), df = 10),
       dt(seq(-5,5, by = .01), df = 15),
       dt(seq(-5,5, by = .01), df = 25),
       dnorm(seq(-5,5, by = .01)))

Distribution <- c(rep("df =  2", length(x)/6),
          rep("df =  5", length(x)/6),
          rep("df = 10", length(x)/6),
          rep("df = 15", length(x)/6),
          rep("df = 25", length(x)/6),
          rep("Unit Normal", length(x)/6))
t_data <- data_frame(x = x, y = y, Distribution = Distribution)

ggplot(t_data, aes(x = x, y = y, group = Distribution, color = Distribution)) + geom_line(aes(linetype = Distribution), size = 1.05) +
  labs(x = "", y = "Density") 

```
    
    
## The t distribution

As $df$ increases, $t$ looks more and more like a normal distribution.

Intuitively, the more data we have, the less error we can have in our estimate of the true population parameter $\sigma$.

With less error, we reduce the need to introduce a conservative adjustment (the fatter tails) in the sampling distribution we use to declare statistical significance.

## The t distribution

Let's take a look at Republican vote share from our example data.

```{r dv, message=FALSE, warning=FALSE}
dat<-read.csv("../Data/mtmv_data_10_12.csv")

n  <- nrow(dat)
M  <- round(mean(dat$vote_share, na.rm = TRUE),3)
s2 <- round(var(dat$vote_share, na.rm = TRUE),3)
s  <- round(sd(dat$vote_share, na.rm = TRUE),3)

ggplot(data = dat, aes(x = vote_share)) + geom_histogram(fill = "firebrick", color = "black") +
  labs(x = "Republican Vote Share (Percent)", y = "Number of Districts")

```


## The t distribution

These data are not normally distributed.  Why doesn't this matter for hypothesis testing?

Summary statistics are:

- $n = `r n`$
- $M = `r M`$
- $s^2 = `r s2`$
- $s = \sqrt{s^2} = `r s`$

```{r setmu}
mu = 52.25
```

Imagine that there is a super-population of US elections in which the mean district Republican vote share is `r mu` percent.  

Can we reject the null hypothesis that this election is like the others in that super-population?


## The t distribution

The $t$ distribution, like the unit normal, is centered on zero.  We therefore convert our estimate of the mean to be a $t$ statistic.  This works just like how we calculated the $z$ statistic:

$$
t = \frac{M - \mu}{\text{SE}}
$$

The only difference is that our estimate of SE uses our sample estimate $s$ in place of $\sigma$.

$$
\text{SE} = \frac{s}{\sqrt{n}} = \frac{`r s`}{\sqrt{`r n`}} = `r round(s/sqrt(n),3)`
$$

## The t distribution
Substituting into our formula for $t$,

$$
t = \frac{`r M` - `r mu`}{`r round(s/sqrt(n),3)`} = `r round((M - mu)/(round(s/sqrt(n),3)),3)`.
$$

Let's compare this to a $t$ distribution with $n - 1 = `r n-1`$ degrees of freedom.

## The t distribution

```{r txmple, message=FALSE, warning=FALSE}

xmin <- -6
xmax <- 6
se   <- round(s/sqrt(n),3)
t    <- round((M - mu)/(round(s/sqrt(n),3)),3)

x = seq(xmin,xmax, by = .01)
y = dt(x, df = n-1)
subset_x <- x[x > t]

t_data <- data_frame(x = x, y = y)

xbar1 <- qt(.025, n-1)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dt(subset_x1, n-1), 0))

xbar2 <- qt(.975, n-1)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dt(subset_x2, n-1), 0))

ggplot(t_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "t", y = "Density", title = paste("df = ",n-1)) +
  geom_segment(aes(x = t, y = 0, xend = t, yend = .08), size = 1.25) + 
  geom_text(x = t, y = .095, label = paste("t = ",t))
```


## The t distribution

We already saw that hypothesis tests depend on sample size.  As $n$ gets larger, SE gets smaller.

SE is part of the formula for $t$, just as it was for $z$, so the same behavior occurs for tests based on $t$.

But sample size also matters because it affects the degrees of freedom, which in turn affect the specific $t$ distribution we utilize.

Keeping $M = `r M`$ and $\mu = `r mu`$, what would be the results with different sample sizes?


## The t distribution

```{r n25, message=FALSE, warning=FALSE}

n = 25

xmin <- -6
xmax <- 6
se   <- round(s/sqrt(n),3)
t    <- round((M - mu)/(round(s/sqrt(n),3)),3)

x = seq(xmin,xmax, by = .01)
y = dt(x, df = n-1)
subset_x <- x[x > t]

t_data <- data_frame(x = x, y = y)

xbar1 <- round(qt(.025, n-1),3)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dt(subset_x1, n-1), 0))

xbar2 <- round(qt(.975, n-1),3)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dt(subset_x2, n-1), 0))

ggplot(t_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "t", y = "Density", title = paste0("n =",n, ", df =", n-1)) +
  geom_segment(aes(x = t, y = 0, xend = t, yend = .1), size = 1.25) + 
  geom_text(x = t, y = .11, label = paste(t)) +
  geom_segment(aes(x = xbar1, y = 0, xend = xbar1, yend = .07), size = 1.25) + 
  geom_text(x = xbar1, y = .08, label = paste("t = ",xbar1)) +
  geom_segment(aes(x = xbar2, y = 0, xend = xbar2, yend = .07), size = 1.25) + 
  geom_text(x = xbar2, y = .08, label = paste("p =", xbar2)) 

```



## The t distribution

```{r n50, message=FALSE, warning=FALSE}

n = 50

xmin <- -6
xmax <- 6
se   <- round(s/sqrt(n),3)
t    <- round((M - mu)/(round(s/sqrt(n),3)),3)

x = seq(xmin,xmax, by = .01)
y = dt(x, df = n-1)
subset_x <- x[x > t]

t_data <- data_frame(x = x, y = y)

xbar1 <- round(qt(.025, n-1),3)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dt(subset_x1, n-1), 0))

xbar2 <- round(qt(.975, n-1),3)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dt(subset_x2, n-1), 0))

ggplot(t_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "t", y = "Density", title = paste0("n =",n, ", df =", n-1)) +
  geom_segment(aes(x = t, y = 0, xend = t, yend = .1), size = 1.25) + 
  geom_text(x = t, y = .11, label = paste(t)) +
  geom_segment(aes(x = xbar1, y = 0, xend = xbar1, yend = .07), size = 1.25) + 
  geom_text(x = xbar1, y = .08, label = paste("t = ",xbar1)) +
  geom_segment(aes(x = xbar2, y = 0, xend = xbar2, yend = .07), size = 1.25) + 
  geom_text(x = xbar2, y = .08, label = paste("p =", xbar2)) 

```



## The t distribution

```{r n100, message=FALSE, warning=FALSE}

n = 100

xmin <- -6
xmax <- 6
se   <- round(s/sqrt(n),3)
t    <- round((M - mu)/(round(s/sqrt(n),3)),3)

x = seq(xmin,xmax, by = .01)
y = dt(x, df = n-1)
subset_x <- x[x > t]

t_data <- data_frame(x = x, y = y)

xbar1 <- round(qt(.025, n-1),3)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dt(subset_x1, n-1), 0))

xbar2 <- round(qt(.975, n-1),3)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dt(subset_x2, n-1), 0))

ggplot(t_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "t", y = "Density", title = paste0("n =",n, ", df =", n-1)) +
  geom_segment(aes(x = t, y = 0, xend = t, yend = .1), size = 1.25) + 
  geom_text(x = t, y = .11, label = paste(t)) +
  geom_segment(aes(x = xbar1, y = 0, xend = xbar1, yend = .07), size = 1.25) + 
  geom_text(x = xbar1, y = .08, label = paste("t = ",xbar1)) +
  geom_segment(aes(x = xbar2, y = 0, xend = xbar2, yend = .07), size = 1.25) + 
  geom_text(x = xbar2, y = .08, label = paste("p =", xbar2)) 

```



## The t distribution

```{r n1000, message=FALSE, warning=FALSE}

n = 1000

xmin <- -6
xmax <- 6
se   <- round(s/sqrt(n),3)
t    <- round((M - mu)/(round(s/sqrt(n),3)),3)

x = seq(xmin,xmax, by = .01)
y = dt(x, df = n-1)
subset_x <- x[x > t]

t_data <- data_frame(x = x, y = y)

xbar1 <- round(qt(.025, n-1),3)
subset_x1 <- x[x < xbar1]
poly_data1 <- data_frame(x_poly = c(xmin, subset_x1, xbar1), y_poly = c(0, dt(subset_x1, n-1), 0))

xbar2 <- round(qt(.975, n-1),3)
subset_x2 <- x[x > xbar2]
poly_data2 <- data_frame(x_poly = c(xbar2, subset_x2, xmax), y_poly = c(0, dt(subset_x2, n-1), 0))

ggplot(t_data, aes(x = x, y = y)) + geom_line() +
  geom_polygon(data = poly_data1, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  geom_polygon(data = poly_data2, aes(x = x_poly, y = y_poly), fill = "firebrick", color = "black") +
  labs(x = "t", y = "Density", title = paste0("n =",n, ", df =", n-1)) +
  geom_segment(aes(x = t, y = 0, xend = t, yend = .1), size = 1.25) + 
  geom_text(x = t, y = .11, label = paste(t)) +
  geom_segment(aes(x = xbar1, y = 0, xend = xbar1, yend = .07), size = 1.25) + 
  geom_text(x = xbar1, y = .08, label = paste("t = ",xbar1)) +
  geom_segment(aes(x = xbar2, y = 0, xend = xbar2, yend = .07), size = 1.25) + 
  geom_text(x = xbar2, y = .08, label = paste("p =", xbar2)) 

```

## Summary

1. Hypothesis testing is done by asserting a null hypothesis and then determining the probability that our sample could have come from the null population.
2. If we don't know the null population's variance, we plug in our sample estimate.
3. The need to estimate the unknown variance introduces an additional source of uncertainty into our hypothesis testing.
4. We therefore use the $t$ distribution -- which has fatter tails than the normal -- to make our tests a bit more conservative.


